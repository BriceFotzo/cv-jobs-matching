{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-28c0fcf9d1e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[0muserN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fotzotalom@gmail.com\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mpswd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"7662;Y@nn\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mmatching_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_link\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muserN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpswd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresume_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"CV_Brice_FOTZO.docx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-28c0fcf9d1e1>\u001b[0m in \u001b[0;36mmatching_pipeline\u001b[1;34m(job_link, user, pwd, resume_path)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmatching_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_link\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresume_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mbrowser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_jobs_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_link\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mjob_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscrape_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[1;31m# keywords=get_keywords_from_job(job_content)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;31m# freq_words=get_n_common_words(keywords,5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common import keys\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "import en_core_web_sm\n",
    "import fr_core_news_sm\n",
    "# Import summarize from gensim\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords# Import the library\n",
    "# to convert MSword doc to txt for processing.\n",
    "import docx2txt\n",
    "nlp_job = fr_core_news_sm.load()\n",
    "nlp_resume=fr_core_news_sm.load()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_jobs_links(job_query,user,pwd):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        job_query ([type]): [description]\n",
    "    \"\"\"\n",
    "    browser=webdriver.Chrome(\"chromedriver.exe\")\n",
    "    browser.get(\"https://www.linkedin.com\")\n",
    "\n",
    "    username=browser.find_element_by_id(\"session_key\")\n",
    "    username.send_keys(user)\n",
    "    password=browser.find_element_by_id(\"session_password\")\n",
    "    password.send_keys(pwd)\n",
    "\n",
    "    login_button=browser.find_element_by_class_name(\"sign-in-form__submit-button\")\n",
    "    login_button.click()\n",
    "\n",
    "    browser.get(job_query)\n",
    "\n",
    "    jobs=browser.find_elements_by_class_name(\"job-card-container\")\n",
    "\n",
    "    jobs_links=[]\n",
    "    for i in jobs:\n",
    "        jobs_links.append(i.find_elements_by_tag_name('a')[0].get_attribute('href'))\n",
    "    return browser,jobs_links\n",
    "\n",
    "\n",
    "def scrape_job(browser,job_link):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    browser.get(job_link)\n",
    "    more_button=browser.find_element_by_class_name(\"artdeco-card__action\")\n",
    "    more_button.click()\n",
    "    sleep(3)\n",
    "    content=browser.find_elements_by_class_name('jobs-box__html-content')[0].text\n",
    "    return content\n",
    "\n",
    "def get_keywords_from_job(job_description):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        job_description ([type]): [description]\n",
    "    \"\"\"\n",
    "    keyword = []\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "    for token in job_description:\n",
    "        if(token.text in stopwords or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    return keyword\n",
    "            \n",
    "def get_n_common_words(keywords,n):\n",
    "    freq_word = Counter(keywords)\n",
    "    print(freq_word.most_common(n))    \n",
    "    return freq_word   \n",
    "\n",
    "def get_sent_strength(freq_word):\n",
    "    sent_strength={}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_strength.keys():\n",
    "                    sent_strength[sent]+=freq_word[word.text]\n",
    "                else:\n",
    "                    sent_strength[sent]=freq_word[word.text]\n",
    "    print(sent_strength)\n",
    "    return sent_strength\n",
    "def summurize_sent(sent_strength,ratio=3):\n",
    "    summarized_sentences = nlargest(ratio, sent_strength, key=sent_strength.get)\n",
    "    print(summarized_sentences)\n",
    "    final_sentences = [ w.text for w in summarized_sentences ]\n",
    "    summary = ' '.join(final_sentences)\n",
    "    print(summary)\n",
    "    return summary\n",
    "# max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "# for word in freq_word.keys():  \n",
    "#         freq_word[word] = (freq_word[word]/max_freq)\n",
    "# freq_word.most_common(10)    \n",
    "\n",
    "def match_resume_and_job(job_description, resume):\n",
    "    text_list = [job_description, resume]\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    cv = CountVectorizer()\n",
    "    count_matrix = cv.fit_transform(text_list)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    # get the match percentage\n",
    "    matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
    "    matchPercentage = round(matchPercentage, 2) # round to two decimal\n",
    "    print(\"Your resume matches about \"+ str(matchPercentage)+ \"% of the job description.\")\n",
    "    print(keywords(job_description, ratio=0.25)) \n",
    "    # gives you the keywords of the job description\n",
    "from urllib.parse import quote\n",
    " \n",
    "\n",
    "def matching_pipeline(job_link,user,pwd,resume_path):\n",
    "    browser,links=get_jobs_links(job_link,user,pwd)\n",
    "    job_content=scrape_job(browser,links[0])\n",
    "    # keywords=get_keywords_from_job(job_content)\n",
    "    # freq_words=get_n_common_words(keywords,5)\n",
    "    # sent_strength=get_sent_strength(freq_words)\n",
    "    # summary=summurize_sent(sent_strength)\n",
    "    resume_content = docx2txt.process(\"CV_Brice_FOTZO.docx\")\n",
    "    match_resume_and_job(job_content,resume_content)\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    job_query_online = input(\"Enter Job description : \")\n",
    "    keyword_job=quote(job_query_online)\n",
    "    jobs=\"https://www.linkedin.com/jobs/search/?keywords=\"+keyword_job+\"&location=France\"\n",
    "    userN=\"fotzotalom@gmail.com\"\n",
    "    pswd=\"7662;Y@nn\"  \n",
    "    matching_pipeline(job_link=jobs,user=userN,pwd=pswd,resume_path=\"CV_Brice_FOTZO.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "document = Document('cv_blue.docx')\n",
    "\n",
    "# Dictionary = {‘sea’: “ocean”}\n",
    "\n",
    "list_t=[]\n",
    "for table in document.tables:\n",
    "    print(table)\n",
    "#Now, I would like to navigate, focus on, get to, whatever to the section that has my\n",
    "#single line of text and execute a find/replace using the dictionary above.\n",
    "#then save the document in the usual way.\n",
    "\n",
    "# document.save('/Users/umityalcin/Desktop/Test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "list_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}